{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-Conditioned 28×28 Diffusion (MNIST-style)\n",
        "\n",
        "Minimal notebook to train and demo a tiny text-conditioned diffusion model that generates 28×28 grayscale images (MNIST style). Intended for fast iteration and interview demos; keep batch/steps small for a quick run, or bump them for quality.\n",
        "\n",
        "**Contents**\n",
        "- Optional lightweight installs\n",
        "- Data: MNIST + simple text prompts\n",
        "- Model: small text encoder + UNet with FiLM\n",
        "- Diffusion training loop (classifier-free guidance ready)\n",
        "- Sampling with adjustable guidance scale and step count\n",
        "- Quick visualization grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies (usually available in most ML envs)\n",
        "# !pip install torch torchvision tqdm matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Device / seed\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: MNIST + simple text prompts\n",
        "def prompt_for_label(label: int) -> str:\n",
        "    names = [\n",
        "        \"zero digit\", \"one digit\", \"two digit\", \"three digit\", \"four digit\",\n",
        "        \"five digit\", \"six digit\", \"seven digit\", \"eight digit\", \"nine digit\",\n",
        "    ]\n",
        "    return names[label]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),              # [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,)) # [-1,1] for diffusion target\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_ds = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "print('Train batches:', len(train_loader))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building blocks: time embedding, text encoder, FiLM-UNet\n",
        "\n",
        "@dataclass\n",
        "class DiffusionConfig:\n",
        "    img_size: int = 28\n",
        "    base_channels: int = 32\n",
        "    channel_mults: tuple = (1, 2, 2)\n",
        "    text_dim: int = 128\n",
        "    time_dim: int = 128\n",
        "    num_heads: int = 4\n",
        "    num_layers_text: int = 2\n",
        "    dropout: float = 0.1\n",
        "    timesteps: int = 400\n",
        "    beta_start: float = 1e-4\n",
        "    beta_end: float = 0.02\n",
        "\n",
        "def sinusoidal_time_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(-math.log(10000) * torch.arange(half, device=timesteps.device) / (half - 1))\n",
        "    args = timesteps[:, None] * freqs[None, :]\n",
        "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "    return emb\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, num_layers: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.proj = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "    def forward(self, tokens, lengths):\n",
        "        x = self.embedding(tokens)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out, _ = self.gru(packed)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        # take last valid timestep per sequence\n",
        "        idx = (lengths - 1).clamp(min=0)\n",
        "        last = out[torch.arange(out.size(0)), idx]\n",
        "        return self.proj(last)\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim * 2)\n",
        "    def forward(self, x, cond):\n",
        "        scale, shift = self.linear(cond).chunk(2, dim=1)\n",
        "        return x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim, text_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.time_film = FiLM(time_dim, out_ch)\n",
        "        self.text_film = FiLM(text_dim, out_ch)\n",
        "        self.act = nn.SiLU()\n",
        "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "    def forward(self, x, t_emb, txt_emb):\n",
        "        h = self.act(self.norm1(self.conv1(x)))\n",
        "        h = self.time_film(h, t_emb)\n",
        "        h = self.text_film(h, txt_emb)\n",
        "        h = self.act(self.norm2(self.conv2(h)))\n",
        "        return h + self.skip(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, cfg: DiffusionConfig, text_vocab: int):\n",
        "        super().__init__()\n",
        "        ch = cfg.base_channels\n",
        "        self.text_encoder = TextEncoder(text_vocab, emb_dim=cfg.text_dim, hidden_dim=cfg.text_dim, num_layers=cfg.num_layers_text, dropout=cfg.dropout)\n",
        "        self.null_text = nn.Parameter(torch.zeros(cfg.text_dim))\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.time_dim, cfg.time_dim * 4), nn.SiLU(), nn.Linear(cfg.time_dim * 4, cfg.time_dim)\n",
        "        )\n",
        "\n",
        "        # Down\n",
        "        self.enc1 = ResBlock(1, ch, cfg.time_dim, cfg.text_dim)\n",
        "        self.enc2 = ResBlock(ch, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
        "        self.enc3 = ResBlock(ch * cfg.channel_mults[1], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.mid = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
        "\n",
        "        # Up\n",
        "        self.up1 = nn.ConvTranspose2d(ch * cfg.channel_mults[2], ch * cfg.channel_mults[1], 2, stride=2)\n",
        "        self.dec1 = ResBlock(ch * cfg.channel_mults[1] * 2, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
        "        self.up2 = nn.ConvTranspose2d(ch * cfg.channel_mults[1], ch, 2, stride=2)\n",
        "        self.dec2 = ResBlock(ch * 2, ch, cfg.time_dim, cfg.text_dim)\n",
        "\n",
        "        self.out = nn.Conv2d(ch, 1, 1)\n",
        "\n",
        "    def forward(self, x, t, txt_tokens, txt_lens, drop_text_prob: float = 0.1):\n",
        "        t_emb = self.time_mlp(sinusoidal_time_embedding(t, self.time_mlp[0].in_features))\n",
        "        txt_emb = self.text_encoder(txt_tokens, txt_lens)\n",
        "        if self.training and drop_text_prob > 0:\n",
        "            mask = (torch.rand(txt_emb.size(0), device=txt_emb.device) < drop_text_prob).float()[:, None]\n",
        "            txt_emb = txt_emb * (1 - mask) + self.null_text[None, :] * mask\n",
        "\n",
        "        e1 = self.enc1(x, t_emb, txt_emb)\n",
        "        e2 = self.enc2(self.pool(e1), t_emb, txt_emb)\n",
        "        e3 = self.enc3(self.pool(e2), t_emb, txt_emb)\n",
        "\n",
        "        m = self.mid(e3, t_emb, txt_emb)\n",
        "\n",
        "        d1 = self.up1(m)\n",
        "        d1 = torch.cat([d1, e2], dim=1)\n",
        "        d1 = self.dec1(d1, t_emb, txt_emb)\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, e1], dim=1)\n",
        "        d2 = self.dec2(d2, t_emb, txt_emb)\n",
        "        return self.out(d2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer helpers (simple character-level for the few prompts we have)\n",
        "\n",
        "class SimpleCharTokenizer:\n",
        "    def __init__(self, texts, pad_token='<pad>', unk_token='<unk>'):\n",
        "        chars = sorted(list({c for t in texts for c in t.lower()}))\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.itos = [pad_token, unk_token] + chars\n",
        "        self.stoi = {c: i for i, c in enumerate(self.itos)}\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.itos)\n",
        "    def encode(self, text: str, max_len: int = 24):\n",
        "        text = text.lower()\n",
        "        ids = [self.stoi.get(c, self.stoi[self.unk_token]) for c in text[:max_len]]\n",
        "        length = len(ids)\n",
        "        if length < max_len:\n",
        "            ids += [self.stoi[self.pad_token]] * (max_len - length)\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
        "\n",
        "all_prompts = [prompt_for_label(i) for i in range(10)]\n",
        "tokenizer = SimpleCharTokenizer(all_prompts)\n",
        "print('Vocab size:', tokenizer.vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusion utilities\n",
        "\n",
        "class SimpleDiffusion(nn.Module):\n",
        "    \"\"\"DDPM utilities with schedule tensors registered as buffers (so .to(device) works).\"\"\"\n",
        "    def __init__(self, cfg: DiffusionConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.timesteps, dtype=torch.float32)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cum = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas', alphas)\n",
        "        self.register_buffer('alphas_cum', alphas_cum)\n",
        "\n",
        "    def sample_timesteps(self, batch_size: int, device: Optional[torch.device] = None):\n",
        "        if device is None:\n",
        "            device = self.betas.device\n",
        "        return torch.randint(0, self.cfg.timesteps, (batch_size,), device=device)\n",
        "\n",
        "    def add_noise(self, x0, t, noise):\n",
        "        # t: (B,) long on same device as buffers\n",
        "        sqrt_ac = self.alphas_cum[t].sqrt()[:, None, None, None]\n",
        "        sqrt_one_minus_ac = (1 - self.alphas_cum[t]).sqrt()[:, None, None, None]\n",
        "        return sqrt_ac * x0 + sqrt_one_minus_ac * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float = 2.0):\n",
        "        # classifier-free guidance\n",
        "        t_batch = torch.full((x.size(0),), t, device=x.device, dtype=torch.long)\n",
        "        eps_text = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=0.0)\n",
        "        eps_null = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=1.0)\n",
        "        eps = eps_null + guidance_scale * (eps_text - eps_null)\n",
        "\n",
        "        beta_t = self.betas[t]\n",
        "        alpha_t = self.alphas[t]\n",
        "        alpha_cum_t = self.alphas_cum[t]\n",
        "\n",
        "        mean = (1 / alpha_t.sqrt()) * (x - beta_t / (1 - alpha_cum_t).sqrt() * eps)\n",
        "        if t == 0:\n",
        "            return mean\n",
        "        noise = torch.randn_like(x)\n",
        "        return mean + beta_t.sqrt() * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, txt_tokens, txt_lens, steps: int = 40, guidance_scale: float = 2.0):\n",
        "        model.eval()\n",
        "        x = torch.randn(txt_tokens.size(0), 1, self.cfg.img_size, self.cfg.img_size, device=txt_tokens.device)\n",
        "        # map steps to actual diffusion steps (stride)\n",
        "        stride = max(1, self.cfg.timesteps // steps)\n",
        "        for t in range(self.cfg.timesteps - 1, -1, -stride):\n",
        "            x = self.p_sample(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
        "        return x\n",
        "\n",
        "\n",
        "def prepare_text_batch(labels):\n",
        "    tokens = []\n",
        "    lengths = []\n",
        "    for y in labels:\n",
        "        tok, l = tokenizer.encode(prompt_for_label(int(y)))\n",
        "        tokens.append(tok)\n",
        "        lengths.append(l)\n",
        "    tokens = torch.stack(tokens).to(device)\n",
        "    lengths = torch.stack(lengths).to(device)\n",
        "    return tokens, lengths\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model + training loop\n",
        "\n",
        "cfg = DiffusionConfig()\n",
        "diffusion = SimpleDiffusion(cfg).to(device)  # <-- move diffusion schedule tensors to GPU\n",
        "model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "num_steps = 800  # keep small for a demo; increase for quality\n",
        "log_interval = 100\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "pbar = tqdm(total=num_steps, desc='train')\n",
        "while step < num_steps:\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        tokens, lens = prepare_text_batch(y)\n",
        "\n",
        "        t = diffusion.sample_timesteps(x.size(0), device)\n",
        "        noise = torch.randn_like(x)\n",
        "        x_noisy = diffusion.add_noise(x, t, noise)\n",
        "\n",
        "        pred = model(x_noisy, t, tokens, lens, drop_text_prob=0.1)\n",
        "        loss = F.mse_loss(pred, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "        pbar.update(1)\n",
        "        if step % log_interval == 0:\n",
        "            pbar.set_postfix(loss=float(loss.detach()))\n",
        "        if step >= num_steps:\n",
        "            break\n",
        "pbar.close()\n",
        "print('Finished training steps:', step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sampling demo\n",
        "model.eval()\n",
        "prompts = [\"zero digit\", \"three digit\", \"five digit\", \"seven digit\", \"nine digit\"]\n",
        "\n",
        "tokens = []\n",
        "lens = []\n",
        "for p in prompts:\n",
        "    tok, l = tokenizer.encode(p)\n",
        "    tokens.append(tok)\n",
        "    lens.append(l)\n",
        "tokens = torch.stack(tokens).to(device)\n",
        "lens = torch.stack(lens).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    samples = diffusion.sample(model, tokens, lens, steps=40, guidance_scale=2.0)\n",
        "\n",
        "imgs = (samples.clamp(-1, 1) * 0.5 + 0.5).cpu()  # back to [0,1]\n",
        "fig, axes = plt.subplots(1, len(prompts), figsize=(len(prompts)*2, 2))\n",
        "for ax, img, p in zip(axes, imgs, prompts):\n",
        "    ax.imshow(img[0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(p)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips / Next steps\n",
        "- Increase `num_steps`, `base_channels`, and train for more iterations for better quality.\n",
        "- Try more diverse prompts by expanding the tokenizer vocabulary (add your own prompt list).\n",
        "- Save/Load: `torch.save(model.state_dict(), 'cfdiffusion.pt')` and load with `model.load_state_dict(torch.load(...))`.\n",
        "- Swap sampler stride logic to use full step schedule for best results; current stride ties `steps` to coarse skipping for speed.\n",
        "- To condition on richer text, swap the GRU for a tiny Transformer encoder.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
